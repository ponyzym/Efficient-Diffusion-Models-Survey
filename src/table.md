## Part 1: O1 Replication
|Title|Venue|Date|
|:---|:---|:---|
|[Imitate, Explore, and Self-Improve: A Reproduction Report on Slow-thinking Reasoning Systems](https://arxiv.org/abs/2412.09413)|arXiv|2024-12|
|[o1-Coder: an o1 Replication for Coding](https://arxiv.org/abs/2412.00154)|arXiv|2024-12|
|[Enhancing LLM Reasoning with Reward-guided Tree Search](https://arxiv.org/abs/2411.11694)|arXiv|2024-11|
|[Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions](https://arxiv.org/abs/2411.14405)|arXiv|2024-11|
|[O1 Replication Journey--Part 2: Surpassing O1-preview through Simple Distillation, Big Progress or Bitter Lesson?](https://arxiv.org/abs/2411.16489)|arXiv|2024-11|
|[O1 Replication Journey: A Strategic Progress Report -- Part 1](https://arxiv.org/abs/2410.18982)|arXiv|2024-10|
## Part 2: Process Reward Models
|Title|Venue|Date|
|:---|:---|:---|
|[PRMBench: A Fine-grained and Challenging Benchmark for Process-Level Reward Models.](https://arxiv.org/abs/2501.03124)|arXiv|2025-01|
|[ReARTeR: Retrieval-Augmented Reasoning with Trustworthy Process Rewarding](https://arxiv.org/abs/2501.07861)|arXiv|2025-01|
|[The Lessons of Developing Process Reward Models in Mathematical Reasoning.](https://arxiv.org/abs/2501.07301)|arXiv|2025-01|
|[ToolComp: A Multi-Tool Reasoning & Process Supervision Benchmark.](https://arxiv.org/abs/2501.01290)|arXiv|2025-01|
|[AutoPSV: Automated Process-Supervised Verifier](https://openreview.net/forum?id=eOAPWWOGs9)|NeurIPS|2024-12|
|[ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search](https://openreview.net/forum?id=8rcFOqEud5)|NeurIPS|2024-12|
|[Free Process Rewards without Process Labels.](https://arxiv.org/abs/2412.01981)|arXiv|2024-12|
|[Outcome-Refining Process Supervision for Code Generation](https://arxiv.org/abs/2412.15118)|arXiv|2024-12|
|[Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations](https://aclanthology.org/2024.acl-long.510/)|ACL|2024-08|
|[OVM: Outcome-supervised Value Models for Planning in Mathematical Reasoning](https://aclanthology.org/2024.findings-naacl.55/)|ACL Findings|2024-08|
|[Step-DPO: Step-wise Preference Optimization for Long-chain Reasoning of LLMs](https://arxiv.org/abs/2406.18629)|arXiv|2024-06|
|[Let's Verify Step by Step.](https://arxiv.org/abs/2305.20050)|ICLR|2024-05|
|[Improve Mathematical Reasoning in Language Models by Automated Process Supervision](https://arxiv.org/abs/2306.05372)|arXiv|2023-06|
|[Making Large Language Models Better Reasoners with Step-Aware Verifier](https://arxiv.org/abs/2206.02336)|arXiv|2023-06|
|[Solving Math Word Problems with Process and Outcome-Based Feedback](https://arxiv.org/abs/2211.14275)|arXiv|2022-11|
## Part 3: Reinforcement Learning
|Title|Venue|Date|
|:---|:---|:---|
|[Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search](https://arxiv.org/abs/2502.02508)|arXiv|2025-02|
|[Advancing Language Model Reasoning through Reinforcement Learning and Inference Scaling](https://arxiv.org/abs/2501.11651)|arXiv|2025-01|
|[Challenges in Ensuring AI Safety in DeepSeek-R1 Models: The Shortcomings of Reinforcement Learning Strategies](https://arxiv.org/abs/2501.17030)|arXiv|2025-01|
|[DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/abs/2501.12948)|arXiv|2025-01|
|[Kimi k1.5: Scaling Reinforcement Learning with LLMs](https://arxiv.org/abs/2501.12599)|arXiv|2025-01|
|[Does RLHF Scale? Exploring the Impacts From Data, Model, and Method](https://arxiv.org/abs/2412.06000)|arXiv|2024-12|
|[Offline Reinforcement Learning for LLM Multi-Step Reasoning](https://arxiv.org/abs/2412.16145)|arXiv|2024-12|
|[ReFT: Representation Finetuning for Language Models](https://aclanthology.org/2024.acl-long.410.pdf)|ACL|2024-08|
|[Deepseekmath: Pushing the limits of mathematical reasoning in open language models](https://arxiv.org/abs/2402.03300)|arXiv|2024-02|
## Part 4: MCTS/Tree Search
|Title|Venue|Date|
|:---|:---|:---|
|[On the Convergence Rate of MCTS for the Optimal Value Estimation in Markov Decision Processes](https://ieeexplore.ieee.org/abstract/document/10870057/)|IEEE TAC|2025-01|
|[Search-o1: Agentic Search-Enhanced Large Reasoning Models](https://arxiv.org/abs/2501.05366)|arXiv|2025-01|
|[rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking](https://arxiv.org/abs/2501.04519)|arXiv|2025-01|
|[ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search](https://arxiv.org/abs/2406.03816)|NeurIPS|2024-12|
|[Forest-of-Thought: Scaling Test-Time Compute for Enhancing LLM Reasoning](https://arxiv.org/abs/2412.09078)|arXiv|2024-12|
|[HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs](https://arxiv.org/abs/2412.18925)|arXiv|2024-12|
|[Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via Collective Monte Carlo Tree Search](https://arxiv.org/abs/2412.18319)|arXiv|2024-12|
|[Proposing and solving olympiad geometry with guided tree search](https://arxiv.org/abs/2412.10673)|arXiv|2024-12|
|[SPaR: Self-Play with Tree-Search Refinement to Improve Instruction-Following in Large Language Models](https://arxiv.org/abs/2412.11605)|arXiv|2024-12|
|[Towards Intrinsic Self-Correction Enhancement in Monte Carlo Tree Search Boosted Reasoning via Iterative Preference Learning](https://arxiv.org/abs/2412.17397)|arXiv|2024-12|
|[CodeTree: Agent-guided Tree Search for Code Generation with Large Language Models](https://arxiv.org/abs/2411.04329)|arXiv|2024-11|
|[GPT-Guided Monte Carlo Tree Search for Symbolic Regression in Financial Fraud Detection](https://arxiv.org/abs/2411.04459)|arXiv|2024-11|
|[MC-NEST -- Enhancing Mathematical Reasoning in Large Language Models with a Monte Carlo Nash Equilibrium Self-Refine Tree](https://arxiv.org/abs/2411.15645)|arXiv|2024-11|
|[Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions](https://arxiv.org/abs/2411.14405)|arXiv|2024-11|
|[SRA-MCTS: Self-driven Reasoning Augmentation with Monte Carlo Tree Search for Code Generation](https://arxiv.org/abs/2411.11053)|arXiv|2024-11|
|[Donâ€™t throw away your value model! Generating more preferable text with Value-Guided Monte-Carlo Tree Search decoding](https://openreview.net/forum?id=kh9Zt2Ldmn#discussion)|CoLM|2024-10|
|[AFlow: Automating Agentic Workflow Generation](https://arxiv.org/abs/2410.10762)|arXiv|2024-10|
|[Interpretable Contrastive Monte Carlo Tree Search Reasoning](https://arxiv.org/abs/2410.01707)|arXiv|2024-10|
|[LLaMA-Berry: Pairwise Optimization for O1-like Olympiad-Level Mathematical Reasoning](https://arxiv.org/abs/2410.02884)|arXiv|2024-10|
|[Towards Self-Improvement of LLMs via MCTS: Leveraging Stepwise Knowledge with Curriculum Preference Learning](https://arxiv.org/abs/2410.06508)|arXiv|2024-10|
|[TreeBoN: Enhancing Inference-Time Alignment with Speculative Tree-Search and Best-of-N Sampling](https://arxiv.org/abs/2410.16033)|arXiv|2024-10|
|[Understanding When Tree of Thoughts Succeeds: Larger Models Excel in Generation, Not Discrimination](https://arxiv.org/abs/2410.17820)|arXiv|2024-10|
|[RethinkMCTS: Refining Erroneous Thoughts in Monte Carlo Tree Search for Code Generation](https://arxiv.org/abs/2409.09584)|arXiv|2024-09|
|[Strategist: Learning Strategic Skills by LLMs via Bi-Level Tree Search](https://arxiv.org/abs/2408.10635)|arXiv|2024-08|
|[LiteSearch: Efficacious Tree Search for LLM](https://arxiv.org/abs/2407.00320)|arXiv|2024-07|
|[Tree Search for Language Model Agents](https://arxiv.org/abs/2407.01476)|arXiv|2024-07|
|[Uncertainty-Guided Optimization on Large Language Model Search Trees](https://arxiv.org/abs/2407.03951)|arXiv|2024-07|
|[Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B](https://arxiv.org/abs/2406.07394)|arXiv|2024-06|
|[Beyond A*: Better Planning with Transformers via Search Dynamics Bootstrapping](https://openreview.net/forum?id=rviGTsl0oy)|ICLR WorkShop|2024-05|
|[LLM Reasoners: New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models](https://openreview.net/forum?id=h1mvwbQiXR)|ICLR WorkShop|2024-05|
|[AlphaMath Almost Zero: process Supervision without process](https://arxiv.org/abs/2405.03553)|arXiv|2024-05|
|[Generating Code World Models with Large Language Models Guided by Monte Carlo Tree Search](https://arxiv.org/abs/2405.15383)|arXiv|2024-05|
|[MindStar: Enhancing Math Reasoning in Pre-trained LLMs at Inference Time](https://arxiv.org/abs/2405.16265)|arXiv|2024-05|
|[Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning](https://arxiv.org/abs/2405.00451)|arXiv|2024-05|
|[Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning](https://arxiv.org/abs/2405.00451)|arXiv|2024-05|
|[Stream of Search (SoS): Learning to Search in Language](https://arxiv.org/abs/2404.03683)|arXiv|2024-04|
|[Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing](https://arxiv.org/abs/2404.12253)|arXiv|2024-04|
|[Reasoning with Language Model is Planning with World Model](https://aclanthology.org/2023.emnlp-main.507/)|EMNLP|2023-12|
|[Large Language Models as Commonsense Knowledge for Large-Scale Task Planning](https://proceedings.neurips.cc/paper_files/paper/2023/hash/65a39213d7d0e1eb5d192aa77e77eeb7-Abstract-Conference.html)|NeurIPS|2023-12|
|[ALPHAZERO-LIKE TREE-SEARCH CAN GUIDE LARGE LANGUAGE MODEL DECODING AND TRAINING](https://openreview.net/forum?id=PJfc4x2jXY)|NeurIPS WorkShop|2023-12|
|[Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training](https://openreview.net/forum?id=PJfc4x2jXY)|NeurIPS WorkShop|2023-12|
|[MAKING PPO EVEN BETTER: VALUE-GUIDED MONTE-CARLO TREE SEARCH DECODING](https://arxiv.org/abs/2309.15028)|arXiv|2023-09|
## Part 5: Self-Training / Self-Improve
|Title|Venue|Date|
|:---|:---|:---|
|[Small LLMs Can Master Reasoning with Self-Evolved Deep Thinking (Rstar-Math)](https://arxiv.org/abs/2501.04519)|arXiv|2025-01|
|[ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search](https://arxiv.org/abs/2406.03816)|NeurIPS|2024-12|
|[Recursive Introspection: Teaching Language Model Agents How to Self-Improve](https://openreview.net/forum?id=DRC9pZwBwR)|NeurIPS|2024-12|
|[B-STaR: Monitoring and Balancing Exploration and Exploitation in Self-Taught Reasoner](https://arxiv.org/abs/2412.17256)|arXiv|2024-12|
|[ReST-EM: Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models](https://openreview.net/forum?id=lNAyUngGFK)|TMLR|2024-09|
|[ReFT: Representation Finetuning for Language Models](https://aclanthology.org/2024.acl-long.410.pdf)|ACL|2024-08|
|[Interactive Evolution: A Neural-Symbolic Self-Training Framework for Large Language Models](https://arxiv.org/abs/2406.11736)|arXiv|2024-06|
|[CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing](https://openreview.net/forum?id=Sx038qxjek)|ICLR|2024-05|
|[Enhancing Large Vision Language Models with Self-Training on Image Comprehension](https://arxiv.org/abs/2405.19716)|arXiv|2024-05|
|[Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking](https://arxiv.org/abs/2403.09629)|arXiv|2024-03|
|[V-star: Training Verifiers for Self-Taught Reasoners](https://arxiv.org/abs/2402.06457)|arXiv|2024-02|
|[Self-Refine: Iterative Refinement with Self-Feedback](https://proceedings.neurips.cc/paper_files/paper/2023/hash/91edff07232fb1b55a505a9e9f6c0ff3-Abstract-Conference.html)|NeurIPS|2023-12|
|[ReST: Reinforced Self-Training for Language Modeling](https://arxiv.org/abs/2308.08998)|arXiv|2023-08|
|[STaR: Bootstrapping Reasoning With Reasoning](https://arxiv.org/abs/2203.14465)|NeurIPS2022|2022-05|
|[Expert Iteration: Thinking Fast and Slow with Deep Learning and Tree Search](https://proceedings.neurips.cc/paper/2017/hash/d8e1344e27a5b08cdfd5d027d9b8d6de-Abstract.html)|NeurIPS|2017-12|
## Part 6: Reflection
|Title|Venue|Date|
|:---|:---|:---|
|[HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs](https://arxiv.org/abs/2412.18925)|arXiv|2024-12|
|[AtomThink: A Slow Thinking Framework for Multimodal Mathematical Reasoning](https://arxiv.org/abs/2411.11930)|arXiv|2024-11|
|[LLaVA-o1: Let Vision Language Models Reason Step-by-Step](https://arxiv.org/abs/2411.10440)|arXiv|2024-11|
|[Vision-Language Models Can Self-Improve Reasoning via Reflection](https://arxiv.org/abs/2411.00855)|arXiv|2024-11|
|[Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers](https://arxiv.org/abs/2408.06195)|arXiv|2024-08|
|[Reflection-Tuning: An Approach for Data Recycling](https://arxiv.org/abs/2310.11716)|arXiv|2023-10|
## Part 7: Efficient System2
|Title|Venue|Date|
|:---|:---|:---|
|[O1-Pruner: Length-Harmonizing Fine-Tuning for O1-Like Reasoning Pruning](https://arxiv.org/abs/2501.12570)|arXiv|2025-01|
|[Think More, Hallucinate Less: Mitigating Hallucinations via Dual Process of Fast and Slow Thinking](https://arxiv.org/abs/2501.01306)|arXiv|2025-01|
|[DynaThink: Fast or Slow? A Dynamic Decision-Making Framework for Large Language Models](https://arxiv.org/abs/2407.01009)|EMNLP|2024-12|
|[B-STaR: Monitoring and Balancing Exploration and Exploitation in Self-Taught Reasoner](https://arxiv.org/abs/2412.17256)|arXiv|2024-12|
|[Token-Budget-Aware LLM Reasoning](https://arxiv.org/abs/2412.18547)|arXiv|2024-12|
|[Training Large Language Models to Reason in a Continuous Latent Space](https://arxiv.org/abs/2412.06769)|arXiv|2024-12|
|[Guiding Language Model Reasoning with Planning Tokens](https://arxiv.org/abs/2310.05707)|CoLM|2024-10|
## Part 8: Explainability
|Title|Venue|Date|
|:---|:---|:---|
|[Agents Thinking Fast and Slow: A Talker-Reasoner Architecture](https://openreview.net/forum?id=xPhcP6rbI4)|NeurIPS WorkShop|2024-12|
|[What Happened in LLMs Layers when Trained for Fast vs. Slow Thinking: A Gradient Perspective](https://arxiv.org/abs/2410.23743)|arXiv|2024-10|
|[When a Language Model is Optimized for Reasoning, Does It Still Show Embers of Autoregression? An Analysis of OpenAI o1](https://arxiv.org/abs/2410.01792)|arXiv|2024-10|
|[The Impact of Reasoning Step Length on Large Language Models](https://arxiv.org/abs/2401.04925)|ACL Findings|2024-08|
|[Distilling System 2 into System 1](https://arxiv.org/abs/2407.06023)|arXiv|2024-07|
|[System 2 Attention (is something you might need too)](https://arxiv.org/abs/2311.11829)|arXiv|2023-11|
## Part 9: Multimodal Agent related Slow-Fast System
|Title|Venue|Date|
|:---|:---|:---|
|[Diving into Self-Evolving Training for Multimodal Reasoning](https://arxiv.org/abs/2412.17451)|ICLR|2025-01|
|[Visual Agents as Fast and Slow Thinkers](https://openreview.net/forum?id=ncCuiD3KJQ)|ICLR|2025-01|
|[Virgo: A Preliminary Exploration on Reproducing o1-like MLLM](https://arxiv.org/abs/2501.01904)|arXiv|2025-01|
|[Scaling Inference-Time Search With Vision Value Model for Improved Visual Comprehension](https://arxiv.org/pdf/2412.03704)|arXiv|2024-12|
|[Slow Perception: Let's Perceive Geometric Figures Step-by-Step](https://arxiv.org/abs/2412.20631)|arXiv|2024-12|
|[AtomThink: A Slow Thinking Framework for Multimodal Mathematical Reasoning](https://arxiv.org/abs/2411.11930)|arXiv|2024-11|
|[LLaVA-o1: Let Vision Language Models Reason Step-by-Step](https://arxiv.org/abs/2411.10440)|arXiv|2024-11|
|[Vision-Language Models Can Self-Improve Reasoning via Reflection](https://arxiv.org/abs/2411.00855)|arXiv|2024-11|
## Part 10: Benchmark and Datasets
|Title|Venue|Date|
|:---|:---|:---|
|[PRMBench: A Fine-grained and Challenging Benchmark for Process-Level Reward Models](https://arxiv.org/abs/2501.03124)|arXiv|2025-01|
|[MR-Ben: A Meta-Reasoning Benchmark for Evaluating System-2 Thinking in LLMs](https://openreview.net/forum?id=GN2qbxZlni)|NeurIPS|2024-12|
|[Do NOT Think That Much for 2+3=? On the Overthinking of o1-like LLMs](https://arxiv.org/abs/2412.21187)|arXiv|2024-12|
|[A Preliminary Study of o1 in Medicine: Are We Closer to an AI Doctor?](https://arxiv.org/abs/2409.15277)|arXiv|2024-09|
